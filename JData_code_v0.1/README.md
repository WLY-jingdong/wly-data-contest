    具体思路是这样的，现在的处理的数据只是4月份的交互数据和用户数据。
    为了保证内存不至于占用过大，先将4月份的交互数据的每个用户的交互数据分割成一个个csv文件然后再针对每个csv文件进行处理（这一步在data_clean.py中实现）。
    然后完成文件分割后就可以进行数据清洗，目前的清洗规则只有3条，就是：
    1、清洗没有对8品类商品进行购买，加购物车和关注的用户
    2、清除在10号到15号没有与任何商品发生交互行为的用户
    3、清洗在2016年4月13号后没有和8号品类商品有过交互的用户
    往后可以添加更多的清洗规则，这些实现都在data_clean.py中有注释，目前的实现方式是直接将不符合要求的文件删除，而现在改变策略是直接在文件名上打上标记就行了，这一步已经实现，只是在代码中被写成注释形式。
    完成数据清洗之后就可以划分数据集：
    首先构造训练集，训练集就是我们的（X,Y)，其中一个样本代表一个（用户|商品）对，其中X的构建是从4月1号到4月13号这段时间内抽取特征实现的，目前的特征维度有13维，如下：
    [用户年龄，用户性别，用户等级，用户注册时长，浏览量，购物量，加购物车量，删除购物车量，关注量，点击量，用户商品最早交互时间，最晚交互时间，总交互时间]
    然后label就是[用户|商品]是否在4月13-15号这段时间被购买，如果是，就打上1，否则打上0.
    则最后的得到的一个样本的形式就是：
    [用户ID,商品ID，商品品类ID,商品品牌ID，用户年龄，用户性别，用户等级，用户注册时长，浏览量，购物量，加购物车量，删除购物车量，关注量，点击量，用户商品最早交互时间，最晚交互时间，总交互时间][用户|商品]
    而测试集的构建是从4月1号-4月15号的所有交互信息组成其实现过程和训练集的构造一致，只是没有打label这个过程，形成的一个测试样本如下：
    [用户ID,商品ID，商品品类ID,商品品牌ID，用户年龄，用户性别，用户等级，用户注册时长，浏览量，购物量，加购物车量，删除购物车量，关注量，点击量，用户商品最早交互时间，最晚交互时间，总交互时间]
    这一部分的实现在Multithread_GetData.py文件中有描述和注释。
    最后得到的样本送入LR模型中进行训练，取预测为1的样例，然后再取这些样例中属于8品类的商品的样例，最终得到可上传的结果文件。这部分实现比较简单，就不写代码了，直接参考sklearn的官方文档即可。
